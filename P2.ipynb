{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bigram_corpus(wordlist):\n",
    "#     \"\"\"\n",
    "#     Returns the bigram corpus given by the input [wordlist]\n",
    "    \n",
    "#     get_bigram_corpus creates a dictionary with bigrams from the [wordlist]\n",
    "#     as keys and counts the instances of each bigram to assign values (except\n",
    "#     for the bigram ('.', '<s>') which represents the start of one review and\n",
    "#     the end of another)\n",
    "    \n",
    "#     wordlist: a list of words (strings)\n",
    "#     \"\"\"\n",
    "#     corpus = {}\n",
    "#     for i, word in enumerate(wordlist[1:], start=1):\n",
    "#         if word != '<s>':\n",
    "#             if (wordlist[i-1], word) not in corpus:\n",
    "#                 corpus[(wordlist[i-1], word)] = 1\n",
    "#             else:\n",
    "#                 corpus[(wordlist[i-1], word)] += 1\n",
    "#     return corpus\n",
    "\n",
    "# def get_smooth_bigram_corpus(tokenlist, bigram_corpus):\n",
    "#     \"\"\"\n",
    "#     Returns a dataframe object where the columns and rows are labeled with the tokens\n",
    "#     in [tokenlist] and the elements are the counts of each bigram (defaulting to 1 to handle\n",
    "#     add-one smoothing) in the format (row, column) so that the count for bigram (x, y) is \n",
    "#     found with df.loc[x, by]\n",
    "    \n",
    "#     get_smooth_bigram_corpus also appends the unknown word character [<UNK>] to handle \n",
    "#     unknown words\n",
    "    \n",
    "#     tokenlist: a list of tokens (strings)\n",
    "#     bigram_corpus: a dictionary of bigram:count pairings\n",
    "#     \"\"\"\n",
    "#     tokenlist.append('<UNK>')\n",
    "#     df = pd.DataFrame(1, index = tokenlist, columns = tokenlist) \n",
    "#     for bigram in bigram_corpus:\n",
    "#         df.loc[bigram[0], bigram[1]] += bigram_corpus[bigram]\n",
    "#     return df\n",
    "\n",
    "# def get_smooth_bigram_prob(bigram, smooth_bigram_corpus):\n",
    "#     \"\"\"\n",
    "#     Returns the probability of a given [bigram] on a given [smooth_bigram_corpus]\n",
    "    \n",
    "#     get_smooth_bigram_prob takes the ratio of the value of [bigram] (df.loc(bigram[0], bigram[1]))\n",
    "#     in the table [smooth_bigram_corpus] to the sum of all elements in the same row\n",
    "    \n",
    "#     bigram: a bigram (tuple of strings)\n",
    "#     smooth_bigram_corpus: a dataframe with tokens as row and column names and bigram counts as values\n",
    "#     \"\"\"\n",
    "#     return smooth_bigram_corpus.loc[bigram[0], bigram[1]] / smooth_bigram_corpus.loc[bigram[0]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data_release/train.csv', encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pos_seq</th>\n",
       "      <th>label_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ca n't fail to be entertaining .</td>\n",
       "      <td>['VERB', 'ADV', 'VERB', 'PART', 'VERB', 'ADJ',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much was he going to tell her ?</td>\n",
       "      <td>['ADV', 'ADJ', 'VERB', 'PRON', 'VERB', 'PART',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Up until that news hit the Committee , Don had...</td>\n",
       "      <td>['ADP', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', '...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Could go on to the rugby and go with them coul...</td>\n",
       "      <td>['VERB', 'VERB', 'PART', 'ADP', 'DET', 'NOUN',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally , we went to the office and they gave ...</td>\n",
       "      <td>['ADV', 'PUNCT', 'PRON', 'VERB', 'ADP', 'DET',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It must be shown that the defendant intended (...</td>\n",
       "      <td>['PRON', 'VERB', 'VERB', 'VERB', 'ADP', 'DET',...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Englishman nodded and poured himself more ...</td>\n",
       "      <td>['DET', 'PROPN', 'VERB', 'CCONJ', 'VERB', 'PRO...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I demanded .</td>\n",
       "      <td>['PRON', 'VERB', 'PUNCT']</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is not known is information on the locati...</td>\n",
       "      <td>['NOUN', 'VERB', 'ADV', 'VERB', 'VERB', 'NOUN'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>That 's getting in the</td>\n",
       "      <td>['DET', 'VERB', 'VERB', 'ADP', 'DET']</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>He had trouble finding an appropriate language...</td>\n",
       "      <td>['PRON', 'VERB', 'NOUN', 'VERB', 'DET', 'ADJ',...</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>And yet he 'd disappeared , without trace .</td>\n",
       "      <td>['CCONJ', 'ADV', 'PRON', 'VERB', 'VERB', 'PUNC...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Why did n't you want to buy the same one again...</td>\n",
       "      <td>['ADV', 'VERB', 'ADV', 'PRON', 'VERB', 'PART',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It was a substantial , two-storey , L-shaped h...</td>\n",
       "      <td>['PRON', 'VERB', 'DET', 'ADJ', 'PUNCT', 'NOUN'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Now , however , it seems , the wheel has turne...</td>\n",
       "      <td>['ADV', 'PUNCT', 'ADV', 'PUNCT', 'PRON', 'VERB...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>They 've been to his little boy 's primary sch...</td>\n",
       "      <td>['PRON', 'VERB', 'VERB', 'ADP', 'ADJ', 'ADJ', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Isobel wants</td>\n",
       "      <td>['PROPN', 'VERB']</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>They were expensive to buy and came a day late .</td>\n",
       "      <td>['PRON', 'VERB', 'ADJ', 'PART', 'VERB', 'CCONJ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sir David English was not informed of the visi...</td>\n",
       "      <td>['PROPN', 'PROPN', 'PROPN', 'VERB', 'ADV', 'VE...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The outline of the soft parts are also clearly...</td>\n",
       "      <td>['DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', '...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Given the already appalling and growing rate o...</td>\n",
       "      <td>['VERB', 'DET', 'ADV', 'ADJ', 'CCONJ', 'VERB',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Elinor was sometimes at a loss for the right w...</td>\n",
       "      <td>['NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Fortunately , technical developments have ensu...</td>\n",
       "      <td>['ADV', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'VERB'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>He laughed .</td>\n",
       "      <td>['PRON', 'VERB', 'PUNCT']</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>and it does mean that they can relax you know ...</td>\n",
       "      <td>['CCONJ', 'PRON', 'VERB', 'VERB', 'ADP', 'PRON...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>You know I mean other than that , no I mean it...</td>\n",
       "      <td>['PRON', 'VERB', 'PRON', 'VERB', 'ADJ', 'ADP',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>She was in hospital for six weeks , and I was ...</td>\n",
       "      <td>['PRON', 'VERB', 'ADP', 'NOUN', 'ADP', 'NUM', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Task analysis has not and can not be reduced t...</td>\n",
       "      <td>['NOUN', 'NOUN', 'VERB', 'ADV', 'CCONJ', 'VERB...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>It was surely repugnant to commonsense that in...</td>\n",
       "      <td>['PRON', 'VERB', 'ADV', 'ADJ', 'ADP', 'VERB', ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>They did n't want the Yanks stumbling in and s...</td>\n",
       "      <td>['PRON', 'VERB', 'ADV', 'VERB', 'DET', 'PROPN'...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6293</th>\n",
       "      <td>And division is shared by .</td>\n",
       "      <td>['CCONJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'PUNCT']</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6294</th>\n",
       "      <td>Discourses on punishment and dictionary defini...</td>\n",
       "      <td>['NOUN', 'ADP', 'NOUN', 'CCONJ', 'ADJ', 'NOUN'...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6295</th>\n",
       "      <td>This will get modern usage of words .</td>\n",
       "      <td>['DET', 'VERB', 'VERB', 'ADJ', 'NOUN', 'ADP', ...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6296</th>\n",
       "      <td>Daddy 's got the marge .</td>\n",
       "      <td>['NOUN', 'PART', 'VERB', 'DET', 'NOUN', 'PUNCT']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>The regulations have obvious implications for ...</td>\n",
       "      <td>['DET', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'ADP', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>The contents of this chapter have scarcely tou...</td>\n",
       "      <td>['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>They 've both got whole</td>\n",
       "      <td>['PRON', 'VERB', 'DET', 'VERB', 'ADJ']</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>Even that Bob Monkhouse opened up quite</td>\n",
       "      <td>['ADV', 'ADP', 'PROPN', 'PROPN', 'VERB', 'PART...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>It was , he thought , an interesting choice of...</td>\n",
       "      <td>['PRON', 'VERB', 'PUNCT', 'PRON', 'VERB', 'PUN...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>More than one child in three is born into a si...</td>\n",
       "      <td>['ADJ', 'ADP', 'NUM', 'NOUN', 'ADP', 'NUM', 'V...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6303</th>\n",
       "      <td>\" No , thank you all the same . \"</td>\n",
       "      <td>['PUNCT', 'INTJ', 'PUNCT', 'VERB', 'PRON', 'AD...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6304</th>\n",
       "      <td>Research on natural hazards has a long traditi...</td>\n",
       "      <td>['NOUN', 'ADP', 'ADJ', 'NOUN', 'VERB', 'DET', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>Seven fifty I think .</td>\n",
       "      <td>['NUM', 'NUM', 'PRON', 'VERB', 'PUNCT']</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>IN THE small Devon town of South Molton , Juli...</td>\n",
       "      <td>['ADP', 'DET', 'ADJ', 'PROPN', 'NOUN', 'ADP', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>Perhaps you share it .</td>\n",
       "      <td>['ADV', 'PRON', 'VERB', 'PRON', 'PUNCT']</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6308</th>\n",
       "      <td>Gone a funny shape .</td>\n",
       "      <td>['VERB', 'DET', 'ADJ', 'NOUN', 'PUNCT']</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6309</th>\n",
       "      <td>The influence of Central Office toned up the r...</td>\n",
       "      <td>['DET', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'VERB...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310</th>\n",
       "      <td>You did n't watch it last night .</td>\n",
       "      <td>['PRON', 'VERB', 'ADV', 'VERB', 'PRON', 'ADJ',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>how much , how much , I do n't really know .</td>\n",
       "      <td>['ADV', 'ADJ', 'PUNCT', 'ADV', 'ADJ', 'PUNCT',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6312</th>\n",
       "      <td>Well it can hear us all then .</td>\n",
       "      <td>['INTJ', 'PRON', 'VERB', 'VERB', 'PRON', 'DET'...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6313</th>\n",
       "      <td>Floor you know you had a middle floor .</td>\n",
       "      <td>['NOUN', 'PRON', 'VERB', 'PRON', 'VERB', 'DET'...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6314</th>\n",
       "      <td>. Well it 's make your mind up time .</td>\n",
       "      <td>['PUNCT', 'INTJ', 'PRON', 'VERB', 'VERB', 'ADJ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6315</th>\n",
       "      <td>But Paul Davis remains unfit .</td>\n",
       "      <td>['CCONJ', 'PROPN', 'PROPN', 'VERB', 'ADJ', 'PU...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>The justification for pouring out large sums w...</td>\n",
       "      <td>['DET', 'NOUN', 'ADP', 'VERB', 'PART', 'ADJ', ...</td>\n",
       "      <td>[0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>Play with the other one !</td>\n",
       "      <td>['VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>In a voice of soft persuasion , she said , Wil...</td>\n",
       "      <td>['ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>It is a symptom of public anxiety about urban ...</td>\n",
       "      <td>['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>I do n't like Miss Fitch .</td>\n",
       "      <td>['PRON', 'VERB', 'ADV', 'VERB', 'PROPN', 'PROP...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6321</th>\n",
       "      <td>A fern-like plant , beautifully preserved in a...</td>\n",
       "      <td>['DET', 'ADJ', 'NOUN', 'PUNCT', 'ADV', 'VERB',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6322</th>\n",
       "      <td>And there were never more than a few dozen rin...</td>\n",
       "      <td>['CCONJ', 'ADV', 'VERB', 'ADV', 'ADJ', 'ADP', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6323 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0                      Ca n't fail to be entertaining .   \n",
       "1                   How much was he going to tell her ?   \n",
       "2     Up until that news hit the Committee , Don had...   \n",
       "3     Could go on to the rugby and go with them coul...   \n",
       "4     Finally , we went to the office and they gave ...   \n",
       "...                                                 ...   \n",
       "6318  In a voice of soft persuasion , she said , Wil...   \n",
       "6319  It is a symptom of public anxiety about urban ...   \n",
       "6320                         I do n't like Miss Fitch .   \n",
       "6321  A fern-like plant , beautifully preserved in a...   \n",
       "6322  And there were never more than a few dozen rin...   \n",
       "\n",
       "                                                pos_seq  \\\n",
       "0     ['VERB', 'ADV', 'VERB', 'PART', 'VERB', 'ADJ',...   \n",
       "1     ['ADV', 'ADJ', 'VERB', 'PRON', 'VERB', 'PART',...   \n",
       "2     ['ADP', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', '...   \n",
       "3     ['VERB', 'VERB', 'PART', 'ADP', 'DET', 'NOUN',...   \n",
       "4     ['ADV', 'PUNCT', 'PRON', 'VERB', 'ADP', 'DET',...   \n",
       "...                                                 ...   \n",
       "6318  ['ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '...   \n",
       "6319  ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', ...   \n",
       "6320  ['PRON', 'VERB', 'ADV', 'VERB', 'PROPN', 'PROP...   \n",
       "6321  ['DET', 'ADJ', 'NOUN', 'PUNCT', 'ADV', 'VERB',...   \n",
       "6322  ['CCONJ', 'ADV', 'VERB', 'ADV', 'ADJ', 'ADP', ...   \n",
       "\n",
       "                                              label_seq  \n",
       "0                                 [0, 0, 0, 0, 0, 0, 0]  \n",
       "1                           [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2     [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "6318  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6319  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6320                              [0, 0, 0, 0, 0, 0, 0]  \n",
       "6321                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "6322  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[6323 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HMM():\n",
    "#     def __init__(self, training_df):\n",
    "#         self.bigram_counts = {}\n",
    "#         self.POS_counts = Counter()\n",
    "        \n",
    "#         self.lex_gen = {}\n",
    "#         self.word_pattern = re.compile(\"(\\w+|<s> |[,.!?;\\(\\)])\")\n",
    "\n",
    "#         self.training_df = training_df\n",
    "            \n",
    "#         for row in self.training_df.iterrows():\n",
    " \n",
    "#             POS_list = ast.literal_eval(row[1][1])\n",
    "#             POS_list.insert(0, '<START>')\n",
    "            \n",
    "#             POS_bigram_counts = get_bigram_corpus(POS_list)\n",
    "            \n",
    "#             sentence_list = row[1][0].split()\n",
    "#             sentence_list.insert(0, '<s>')\n",
    "            \n",
    "            \n",
    "#             for i, word in enumerate(sentence_list):\n",
    "#                 if word not in self.lex_gen:\n",
    "#                     self.lex_gen[word] = {POS_list[i] : 1}\n",
    "#                 else:\n",
    "#                     if POS_list[i] not in self.lex_gen[word]:\n",
    "#                         self.lex_gen[word][POS_list[i]] = 1\n",
    "#                     else:\n",
    "#                         self.lex_gen[word][POS_list[i]] += 1\n",
    "            \n",
    "#             # Updates list of POS counts across whole training set \n",
    "#             self.POS_counts.update(POS_list)\n",
    "            \n",
    "#             for bigram in POS_bigram_counts: \n",
    "#                 if bigram in self.bigram_counts:\n",
    "#                     self.bigram_counts[bigram] += POS_bigram_counts[bigram]\n",
    "#                 else:\n",
    "#                     self.bigram_counts[bigram] = POS_bigram_counts[bigram]\n",
    "        \n",
    "#         self.smoothed_bigram_counts_df = get_smooth_bigram_corpus(list(self.POS_counts), self.bigram_counts)\n",
    "    \n",
    "\n",
    "\n",
    "      \n",
    "#     def prob_tagged_sequence(self, sequence):\n",
    "#         '''\n",
    "        \n",
    "#         sequence: tuple where first element is sentence as a string, second element is a \n",
    "#             list of POS associated with each word of sentence \n",
    "#         '''\n",
    "        \n",
    "#         sentence_list = self.word_pattern.findall(sequence[0])\n",
    "#         POS_list = sequence[1]\n",
    "        \n",
    "#         log_prob_acc = 0\n",
    "\n",
    "#         for i, POS in enumerate(POS_list):\n",
    "#             if i == 0:\n",
    "#                 continue\n",
    "\n",
    "#             POS_bigram = (POS_list[i-1], POS) \n",
    "#             log_prob_acc += math.log(get_smooth_bigram_prob(POS_bigram, self.smoothed_bigram_counts_df))\n",
    "\n",
    "#             log_prob_acc += math.log(self.lex_gen[sentence_list[i]].get(POS, 1)/dict(self.POS_counts)[POS]) \n",
    "        \n",
    "#         return math.exp(log_prob_acc)\n",
    "    \n",
    "    \n",
    "#     def viterbi(self, sentence):\n",
    "#         '''\n",
    "#         sentence: string\n",
    "#         '''\n",
    "        \n",
    "#         sentence_list = self.word_pattern.findall(sentence)\n",
    "\n",
    "#         backpointers = []\n",
    "#         scores = []\n",
    "#         previous_scores = []\n",
    "\n",
    "#         POS_list = list(self.POS_counts)\n",
    "#         num_lex_categories = len(self.POS_counts)\n",
    "\n",
    "#         #initialization\n",
    "#         for i in range(1, num_lex_categories):\n",
    "#             POS = POS_list[i]\n",
    "\n",
    "#             initial_prob = get_smooth_bigram_prob(('<START>', POS), self.smoothed_bigram_counts_df)\n",
    "#             lex_gen_prob = self.lex_gen.get(sentence_list[0], '<UNK>').get(POS, 1)/self.POS_counts.get(POS)\n",
    "\n",
    "#             previous_scores.append(initial_prob * lex_gen_prob)\n",
    "\n",
    "#         #iteration\n",
    "#         for t in range(1, len(sentence_list)):\n",
    "\n",
    "#             t_backpointers = []\n",
    "\n",
    "#             for i in range(1, num_lex_categories):\n",
    "\n",
    "#                 temp_backpointer = None\n",
    "#                 max_score = (0, -1)\n",
    "\n",
    "#                 for j in range(1, num_lex_categories):\n",
    "#                     transition_prob = get_smooth_bigram_prob((POS_list[j], POS_list[i]), self.smoothed_bigram_counts_df) \n",
    "#                     lex_gen_prob = self.lex_gen.get(sentence_list[t], '<UNK>').get(POS, 1) / self.POS_counts.get(POS)\n",
    "#                     #index j-1 accounts for exclusion of START POS during init of previous_scores\n",
    "#                     score = previous_scores[j-1] * transition_prob * lex_gen_prob\n",
    "#                     scores.append(score)\n",
    "                    \n",
    "#                     print(j)\n",
    "#                     print(score)\n",
    "\n",
    "#                     if score > max_score[0]:\n",
    "#                         max_score = (score, j)\n",
    "#                         temp_backpointer = j\n",
    "\n",
    "#                 t_backpointers.append(temp_backpointer)\n",
    "#                 previous_scores = scores\n",
    "\n",
    "#             backpointers.append(t_backpointers)\n",
    "\n",
    "#         #backtracking\n",
    "#         tags = []\n",
    "#         max_index = max_score[1]\n",
    "#         print(backpointers)\n",
    "#         for bptr_list in reversed(backpointers):\n",
    "#             print(len(bptr_list))\n",
    "#             print(max_index)\n",
    "#             tags.insert(0, POS_list[max_index])\n",
    "            \n",
    "#             #index max_index-1 accounts for exclusion of START POS during init of backpointer lists\n",
    "#             max_index = bptr_list[max_index-1]\n",
    "            \n",
    "#         return tags\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(sentence):\n",
    "#     word_pattern = re.compile(\"(\\w+|<s> |[,.!?;\\(\\)])\")\n",
    "#     return word_pattern.findall(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_Metaphor_Tagger():\n",
    "    def __init__(self, training_data):\n",
    "        self.tag_counts = Counter()\n",
    "        self.tag_bigrams = {}\n",
    "        self.emissions = {}\n",
    "        \n",
    "        #iterate through training examples\n",
    "        for row in training_data.iterrows():\n",
    "            \n",
    "            #preprocess: add start characters and labels for computing initial probabilities\n",
    "            tags_string = row[1][2]\n",
    "            tags = ast.literal_eval(tags_string)\n",
    "            tags.insert(0, '<START>')\n",
    "            sentence = row[1][0].split()\n",
    "            sentence.insert(0, '<s>')\n",
    "            \n",
    "            #get label bigram counts -- (0,0), (0,1), (1,0), (1,1), ('<START>',0), ('<START>',1)\n",
    "            for t in range(1, len(tags)):\n",
    "                tag_bigram = (tags[t-1], tags[t])\n",
    "                if tag_bigram not in self.tag_bigrams:\n",
    "                    self.tag_bigrams[tag_bigram] = 1\n",
    "                else:\n",
    "                    self.tag_bigrams[tag_bigram] += 1\n",
    "                    \n",
    "            #get individual tag counts\n",
    "            self.tag_counts.update(tags)\n",
    "            \n",
    "            #get emission counts\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word not in self.emissions:\n",
    "                    self.emissions[word] = {tags[i] : 1}\n",
    "                else:\n",
    "                    if tags[i] not in self.emissions[word]:\n",
    "                        self.emissions[word][tags[i]] = 1\n",
    "                    else:\n",
    "                        self.emissions[word][tags[i]] += 1\n",
    "\n",
    "        print(self.tag_counts)\n",
    "        print(self.tag_bigrams)\n",
    "\n",
    "      \n",
    "    def prob_tagged_sequence(self, sequence):\n",
    "        '''\n",
    "        sequence: tuple where first element is sentence as a string, second element is a \n",
    "            list of tags associated with each word of sentence \n",
    "            \n",
    "        tags are binary (0 or 1)\n",
    "        '''\n",
    "        sentence = sequence[0].split()\n",
    "        tags = sequence[1]\n",
    "        \n",
    "        #initialize log_prob_acc with initial probability\n",
    "        initial_transition_prob = self.tag_bigrams[('<START>', tags[0])] / self.tag_counts['<START>']\n",
    "        if sentence[0] in self.emissions:\n",
    "            initial_emission_prob = self.emissions[sentence[0]].get(tags[0], 1) / self.tag_counts[tags[0]]\n",
    "        else:\n",
    "            initial_emission_prob = 1 / self.tag_counts[tags[0]]\n",
    "        log_prob_acc = math.log(initial_transition_prob) + math.log(initial_emission_prob)\n",
    "        \n",
    "        #sum log transition and emission probabilities\n",
    "        for t in range(1, len(tags)):\n",
    "            tag_bigram = (tags[t-1], tags[t])\n",
    "            transition_prob = self.tag_bigrams[tag_bigram] / self.tag_counts[tags[t-1]]\n",
    "            if sentence[t] in self.emissions:\n",
    "                emission_prob = self.emissions[sentence[t]].get(tags[t], 1) / self.tag_counts[tags[t]]\n",
    "            else:\n",
    "                emission_prob = 1 / self.tag_counts[tags[t]]\n",
    "            log_prob_acc = math.log(transition_prob) + math.log(emission_prob)\n",
    "        \n",
    "        return math.exp(log_prob_acc)\n",
    "    \n",
    "    \n",
    "    def viterbi(self, sentence):\n",
    "        '''\n",
    "        sentence: string where each token (punctuation included) is separated by a space\n",
    "        '''\n",
    "        sentence = sentence.split()\n",
    "\n",
    "        backpointers = []\n",
    "        scores = []\n",
    "        previous_scores = []\n",
    "\n",
    "        tags = list(self.tag_counts)\n",
    "\n",
    "        #initialization\n",
    "        for t in range(1, len(tags)):\n",
    "            tag = tags[t]\n",
    "\n",
    "            initial_transition_prob = self.tag_bigrams[('<START>', tag)] / self.tag_counts['<START>']\n",
    "            if sentence[0] in self.emissions:\n",
    "                initial_emission_prob = self.emissions[sentence[0]].get(tag, 1) / self.tag_counts[tag]\n",
    "            else:\n",
    "                initial_emission_prob = 1 / self.tag_counts[tag]\n",
    "            \n",
    "            previous_scores.append(initial_transition_prob * initial_emission_prob)\n",
    "            \n",
    "        #iteration\n",
    "        #w is index of current word\n",
    "        for w in range(1, len(sentence)):\n",
    "\n",
    "            w_backpointers = []\n",
    "            \n",
    "            #t is index of current tag\n",
    "            for t in range(1, len(tags)):\n",
    "                \n",
    "                t_backpointer = None\n",
    "                max_score = (0, None)\n",
    "\n",
    "                #j is index of previous tag\n",
    "                for j in range(1, len(tags)):\n",
    "                    \n",
    "                    transition_prob = self.tag_bigrams[(tags[j], tags[t])] / self.tag_counts[tags[j]]\n",
    "                    if sentence[w] in self.emissions:\n",
    "                        emission_prob = self.emissions[sentence[w]].get(tags[t], 1) / self.tag_counts[tags[t]]\n",
    "                    else:\n",
    "                        emission_prob = 1 / self.tag_counts[tags[t]]\n",
    "                    \n",
    "                    score = previous_scores[j-1] * transition_prob * emission_prob                    \n",
    "                    scores.append(score)\n",
    "                    if score > max_score[0]:\n",
    "                        max_score = (score, j)\n",
    "                        t_backpointer = j\n",
    "\n",
    "                w_backpointers.append(t_backpointer)\n",
    "\n",
    "            previous_scores = scores\n",
    "            backpointers.insert(0, w_backpointers)\n",
    "            \n",
    "        #backtracking\n",
    "        max_index = max_score[1]\n",
    "        output = [tags[max_index]]\n",
    "        for bptrs in backpointers:\n",
    "            max_index = bptrs[max_index-1]\n",
    "            output.insert(0, tags[max_index])\n",
    "            \n",
    "        return output\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 103571, 1: 13051, '<START>': 6323})\n",
      "{('<START>', 0): 5830, (0, 0): 87135, (0, 1): 10169, (1, 0): 10606, (1, 1): 2389, ('<START>', 1): 493}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = HMM_Metaphor_Tagger(train_df)\n",
    "# hmm.prob_tagged_sequence(('the', [0]))\n",
    "model_1.viterbi('the man is drowning in debt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

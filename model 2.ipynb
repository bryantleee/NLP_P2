{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data_release/train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Seq Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_seqs_list_of_lists = [pos_seq.split() for pos_seq in train_data['pos_seq']]\n",
    "\n",
    "pos_seqs = []\n",
    "for pos_seq in train_data['pos_seq']:\n",
    "    pos_seqs += pos_seq.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116622, 1)\n"
     ]
    }
   ],
   "source": [
    "pos_model = W2VTransformer(size=1, min_count=1, seed=1)\n",
    "# What is the vector representation of the word 'graph'?\n",
    "posvecs = pos_model.fit(pos_seqs_list_of_lists).transform(pos_seqs)\n",
    "posvecs = np.array(posvecs)\n",
    "print(posvecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word token transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token_list_of_lists = [sentence.lower().split() for sentence in train_data['sentence']]\n",
    "\n",
    "word_tokens = []\n",
    "for sentence in train_data['sentence']:\n",
    "    word_tokens += sentence.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116622, 10)\n"
     ]
    }
   ],
   "source": [
    "word_model = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "# What is the vector representation of the word 'graph'?\n",
    "wordvecs = word_model.fit(word_token_list_of_lists).transform(word_tokens)\n",
    "wordvecs = np.array(wordvecs)\n",
    "print(wordvecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116622, 11)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs = np.concatenate((wordvecs, posvecs), axis=1)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_seqs = []\n",
    "for label_seq in train_data['label_seq']:\n",
    "    label_seqs += ast.literal_eval(label_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(vecs, label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('./data_release/val.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = clf.predict_log_proba(vecs)\n",
    "y = clf.predict(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-4ea432cf685e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "vecs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt_Metaphor_Tagger():\n",
    "    def __init__(self, train_data, size):\n",
    "        '''\n",
    "        train_data: dataframe of word features where the first column is the sentence, second column\n",
    "            is a string of a list of POS_Sequences, and third columnn is a string of a list of metaphor \n",
    "            label sequences (0: not metaphor, 1: metaphor)\n",
    "        '''\n",
    "        self.size = size\n",
    "        \n",
    "        # Transform POS_Seq\n",
    "        self.pos_seqs_list_of_lists = [ast.literal_eval(pos_seq) for pos_seq in train_data['pos_seq']]\n",
    "        self.pos_seqs = []\n",
    "        for pos_seq in train_data['pos_seq']:\n",
    "            self.pos_seqs += ast.literal_eval(pos_seq)\n",
    "        self.pos_model = W2VTransformer(size=1, min_count=1, seed=1)\n",
    "        self.posvecs = np.array(self.pos_model.fit(self.pos_seqs_list_of_lists).transform(self.pos_seqs))\n",
    "        \n",
    "#         print('pos_Seq', self.pos_seqs)\n",
    "        \n",
    "        # Transform Word Tokens\n",
    "        self.word_token_list_of_lists = [sentence.lower().split() for sentence in train_data['sentence']]\n",
    "        self.word_tokens = []\n",
    "        self.sample_indices = []\n",
    "        for i, sentence in enumerate(train_data['sentence']):\n",
    "            self.word_tokens += sentence.lower().split()\n",
    "            self.sample_indices += [i for x in range(len(sentence))]\n",
    "        \n",
    "        self.word_model = W2VTransformer(size=self.size, min_count=1, seed=1)\n",
    "        self.wordvecs = np.array(self.word_model.fit(self.word_token_list_of_lists).transform(self.word_tokens))\n",
    "        \n",
    "        #TF-IDF \n",
    "#         self.tfidf_vectorizer = TfidfVectorizer()\n",
    "#         self.tfidf_vecs = self.tfidf_vectorizer.fit_transform([sentence.lower() for sentence in train_data['sentence']])\n",
    "#         print('tfidf shape:', self.tfidf_vecs.shape)\n",
    "#         print(self.tfidf_vecs[0])\n",
    "#         print('feature names:', self.tfidf_vectorizer.get_feature_names())\n",
    "        \n",
    "        # Concatenate Feature Vectors\n",
    "        self.vecs = np.concatenate((self.wordvecs, self.posvecs), axis=1)\n",
    "        \n",
    "#         for i, word_vec in enumerate(self.vecs):\n",
    "#             word = self.word_tokens[i]\n",
    "#             sentence_index = self.sample_indices[i]\n",
    "#             word_index = self.tfidf_vectorizer.get_feature_names().index(word)\n",
    "#             print('sentence index:', sentence_index)\n",
    "#             print('word index:', word_index)\n",
    "#             print(type(self.tfidf_vecs))\n",
    "#             tfidf = self.tfidf_vecs[sentence_index][word_index]\n",
    "#             word_vec += tfidf\n",
    "        \n",
    "        \n",
    "        # Create Target Vector\n",
    "        self.label_seqs = []\n",
    "        for label_seq in train_data['label_seq']:\n",
    "            self.label_seqs += ast.literal_eval(label_seq)\n",
    "        \n",
    "        # Train MaxEnt classifier\n",
    "        self.classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').\\\n",
    "            fit(self.vecs, self.label_seqs)\n",
    "        \n",
    "        # Get tag bigrams for transition probabilities\n",
    "        self.tag_counts = Counter()\n",
    "        self.tag_bigrams = {}\n",
    "        for row in train_data.iterrows():\n",
    "            \n",
    "            #preprocess: add start characters and labels for computing initial probabilities\n",
    "            # and convert strings to lists and downcase sentences\n",
    "            tags_string = row[1][2]\n",
    "            tags = ast.literal_eval(tags_string)\n",
    "            tags.insert(0, '<START>')\n",
    "            \n",
    "            #get label bigram counts -- (0,0), (0,1), (1,0), (1,1), ('<START>',0), ('<START>',1)\n",
    "            for t in range(1, len(tags)):\n",
    "                tag_bigram = (tags[t-1], tags[t])\n",
    "                if tag_bigram not in self.tag_bigrams:\n",
    "                    self.tag_bigrams[tag_bigram] = 1\n",
    "                else:\n",
    "                    self.tag_bigrams[tag_bigram] += 1\n",
    "                    \n",
    "            #get individual tag counts\n",
    "            self.tag_counts.update(tags)\n",
    "    \n",
    "    def transform_sentence(self, sentence, pos_sequence):\n",
    "        wordvecs = np.zeros(shape=(len(sentence.split()), self.size))\n",
    "        posvecs = np.zeros(shape=(len(pos_sequence), 1))\n",
    "        \n",
    "        for i, word in enumerate(sentence.lower().split()):\n",
    "            if word in self.word_tokens:\n",
    "                wordvecs[i] = self.word_model.transform(word)\n",
    "            else:\n",
    "                wordvecs[i] = np.array([None for i in range(0, self.size)])\n",
    "#                 print(wordvecs[i])\n",
    "            posvecs[i] = self.pos_model.transform(pos_sequence[i])\n",
    "        \n",
    "        \n",
    "        return np.concatenate((wordvecs, posvecs), axis=1)\n",
    "    \n",
    "    def predict_log_proba(self, vectors):\n",
    "        return self.classifier.predict_log_proba(vectors)\n",
    "    \n",
    "    def predict(self, vectors):\n",
    "        return self.classifier.predict(vectors)\n",
    "    \n",
    "    def viterbi(self, feature_vectors):\n",
    "        previous_log_scores = []\n",
    "        backpointers = []\n",
    "        tags = list(self.tag_counts)\n",
    "\n",
    "        #initialization\n",
    "        for t in range(1, len(tags)):\n",
    "            tag = tags[t]\n",
    "            \n",
    "            initial_transition_prob = self.tag_bigrams[('<START>', tag)] / self.tag_counts['<START>']\n",
    "            \n",
    "            if np.isnan(feature_vectors[0]).any():\n",
    "                initial_maxent_log_prob = 0\n",
    "            else:\n",
    "                initial_maxent_log_prob = self.classifier.predict_log_proba([feature_vectors[0]])[0][t-1]\n",
    "            \n",
    "            previous_log_scores.append(math.log(initial_transition_prob) + initial_maxent_log_prob)\n",
    "        \n",
    "        #iteration\n",
    "        #w is index of current word\n",
    "        for w in range(1, feature_vectors.shape[0]):\n",
    "            \n",
    "            log_scores = [None, None]\n",
    "            w_backpointers = []\n",
    "            max_log_score_final = (float('-inf'), None)\n",
    "            \n",
    "            #t is index of current tag\n",
    "            for t in range(1, len(tags)):\n",
    "                \n",
    "                t_backpointer = None\n",
    "                max_log_score = (float('-inf'), None)\n",
    "\n",
    "                #j is index of previous tag\n",
    "                for j in range(1, len(tags)):\n",
    "                    \n",
    "                    transition_prob = self.tag_bigrams[(tags[j], tags[t])] / self.tag_counts[tags[j]]\n",
    "                    if np.isnan(feature_vectors[w]).any():\n",
    "                        maxent_log_prob = 0\n",
    "                    else:\n",
    "                        maxent_log_prob = self.classifier.predict_log_proba([feature_vectors[w]])[0][t-1]\n",
    "                    \n",
    "                    log_score = previous_log_scores[j-1] +  math.log(transition_prob) + maxent_log_prob\n",
    "                    if log_score > max_log_score[0]:\n",
    "                        max_log_score = (log_score, j)\n",
    "                        t_backpointer = j\n",
    "                        \n",
    "                    if max_log_score[0] > max_log_score_final[0]:\n",
    "                        max_log_score_final = max_log_score\n",
    "                    \n",
    "                \n",
    "                log_scores[t-1] = max_log_score[0]\n",
    "                w_backpointers.append(t_backpointer)\n",
    "                \n",
    "            previous_log_scores = log_scores\n",
    "            backpointers.insert(0, w_backpointers)\n",
    "        \n",
    "        #backtracking\n",
    "        max_index = previous_log_scores.index(max(previous_log_scores)) + 1\n",
    "        output = [tags[max_index]]\n",
    "    \n",
    "        if feature_vectors.shape[0] == 1:\n",
    "            return output\n",
    "        \n",
    "        max_index = max_log_score_final[1]\n",
    "        for bptrs in backpointers:\n",
    "            max_index = bptrs[max_index-1]\n",
    "            output.insert(0, tags[max_index])\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent = MaxEnt_Metaphor_Tagger(train_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector = \\\n",
    "    maxent.transform_sentence('he continued , hackles rising .', ['PRON', 'VERB', 'PUNCT', 'NOUN', 'VERB', 'PUNCT'])\n",
    "maxent.viterbi(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('./data_release/val.csv', encoding='latin-1')\n",
    "def validate(model, val_data):\n",
    "    labels = []\n",
    "    for row in val_data.iterrows():\n",
    "        sentence = row[1][0]\n",
    "        feature_vector = model.transform_sentence(sentence, ast.literal_eval(row[1][1]))\n",
    "        labels += model.viterbi(feature_vector)\n",
    "    ids = [i for i in range(len(labels))]\n",
    "    df = pd.DataFrame({'idx': ids, 'label': labels}, columns = ['idx', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38598</th>\n",
       "      <td>38598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38599</th>\n",
       "      <td>38599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38600</th>\n",
       "      <td>38600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38601</th>\n",
       "      <td>38601</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38602</th>\n",
       "      <td>38602</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38603</th>\n",
       "      <td>38603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38604</th>\n",
       "      <td>38604</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38605</th>\n",
       "      <td>38605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38606</th>\n",
       "      <td>38606</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38607</th>\n",
       "      <td>38607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38608</th>\n",
       "      <td>38608</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38609</th>\n",
       "      <td>38609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38610</th>\n",
       "      <td>38610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38611</th>\n",
       "      <td>38611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38612</th>\n",
       "      <td>38612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38613</th>\n",
       "      <td>38613</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38614</th>\n",
       "      <td>38614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38615</th>\n",
       "      <td>38615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38616</th>\n",
       "      <td>38616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38617</th>\n",
       "      <td>38617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38618</th>\n",
       "      <td>38618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38619</th>\n",
       "      <td>38619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38620</th>\n",
       "      <td>38620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38621</th>\n",
       "      <td>38621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38622</th>\n",
       "      <td>38622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38623</th>\n",
       "      <td>38623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38624</th>\n",
       "      <td>38624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38625</th>\n",
       "      <td>38625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38626</th>\n",
       "      <td>38626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38627</th>\n",
       "      <td>38627</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38628 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx  label\n",
       "0          0      0\n",
       "1          1      0\n",
       "2          2      0\n",
       "3          3      0\n",
       "4          4      0\n",
       "...      ...    ...\n",
       "38623  38623      0\n",
       "38624  38624      0\n",
       "38625  38625      0\n",
       "38626  38626      0\n",
       "38627  38627      0\n",
       "\n",
       "[38628 rows x 2 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(maxent, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
